{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512afbd4-9333-499d-8609-3dff0d678cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tomli\n",
    "from cached_path import cached_path\n",
    "\n",
    "from model import DiT, CFM\n",
    "import torch\n",
    "from f5_tts.model.utils import (\n",
    "    get_tokenizer,\n",
    "    convert_char_to_pinyin,\n",
    ")\n",
    "\n",
    "from infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6c0cd-bc17-408d-9893-795b7f5f2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "cfg_strength = 3.0\n",
    "scale_phi = 0.75\n",
    "\n",
    "target_sample_rate = 24000\n",
    "mel_spec_type  = \"vocos\"\n",
    "vocoder_name   = \"vocos\"\n",
    "n_mel_channels = 100\n",
    "win_length     = 1024\n",
    "hop_length     = int(win_length//4)\n",
    "n_fft          = 1024\n",
    "target_rms     = 0.1\n",
    "ode_method     = \"euler\"\n",
    "nfe_step       = 32  # 16, 32\n",
    "speed          = 1.0\n",
    "fix_duration   = None\n",
    "vocab_file     = \"./infer/examples/vocab.txt\"\n",
    "tokenizer      = \"custom\"\n",
    "ode_method     = \"euler\"\n",
    "cross_fade_duration = 0.15\n",
    "sway_sampling_coef = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2c06-878d-4c64-9c14-4af81b12f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "vocoder     = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=\"./vocoder\")\n",
    "transformer = DiT(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce4d68-2a55-4cec-bc6c-1a350bf6211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04907936-41c4-46dc-897b-77cb078e4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "ckpt_path = '/workspace/model_1200000.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "# 키 이름에서 \"emewe_model.\" prefix 제거\n",
    "new_state_dict = {\n",
    "    k.replace(\"ema_model.\", \"\"): v\n",
    "    for k, v in checkpoint['ema_model_state_dict'].items()\n",
    "    if k.startswith(\"ema_model.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41153a95-d121-44e1-afd1-8948941c0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1323c60-73b9-4694-a383-5640d46d5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I thought I saw you in my dream.\", \"Hey how are you?\"]\n",
    "duration = 280\n",
    "audio, sr = torch.ones((2, 24000)), 24000\n",
    "final_text_list = convert_char_to_pinyin(text)\n",
    "print(final_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aff2ac-0535-4f78-b44b-28a70ff801cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, sec = model.sample(\n",
    "    cond=audio.to(device),\n",
    "    text=final_text_list,\n",
    "    duration=duration,\n",
    "    steps=nfe_step,\n",
    "    cfg_strength=cfg_strength,\n",
    "    sway_sampling_coef=sway_sampling_coef,\n",
    "    \n",
    "    duplicate_test=False,\n",
    "    t_inter=None,\n",
    "    no_ref_audio=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e3618-3d9e-46a7-a35b-a98412b611b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generated.to(torch.float32)\n",
    "print(generated.shape)\n",
    "generated_mel_spectrogram = generated.permute(0, 2, 1)\n",
    "generated_wave_form = vocoder.decode(generated_mel_spectrogram)\n",
    "print(generated_wave_form.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191a3d7-8b27-451d-93a6-7f69455e514f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ead2a0-29f5-44bc-812d-505711d52af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파라미터 수\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 학습 가능한(=requires_grad=True) 파라미터 수\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2375f2e-668a-4baa-9611-812d709b0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "ref_audio = \"/workspace/tts/valid_data/monster-saying-i-love-you-sound-effect-234404303_nw_prev.mp3\"\n",
    "ref_text = \"I love you.\"\n",
    "\n",
    "audio, sr = librosa.load(ref_audio, sr=24000, mono=True)\n",
    "audio = torch.tensor(audio).unsqueeze(dim=0)\n",
    "print(audio.shape) # 2.18초 = 약 202 frames\n",
    "\n",
    "text = [ref_text+\"I thought I saw you in my dream. model will be optimized by back propagation with tons of data.\", ref_text+\"Hey how are you?\"]\n",
    "duration = 920\n",
    "final_text_list = convert_char_to_pinyin(text)\n",
    "\n",
    "total = 0\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    generated, sec = model.sample(\n",
    "        cond           = audio.to(device),\n",
    "        text           = final_text_list,\n",
    "        duration       = duration,\n",
    "        steps          = 32,\n",
    "        cfg_strength   = cfg_strength,\n",
    "        sway_sampling_coef=-1,\n",
    "        \n",
    "        duplicate_test = False,\n",
    "        t_inter        = None,\n",
    "        no_ref_audio   = False,\n",
    "        batch_size     = 1\n",
    "    )\n",
    "    \n",
    "    generated = generated.to(torch.float32)\n",
    "    generated_mel_spectrogram = generated.permute(0, 2, 1)\n",
    "    generated_wave_form = vocoder.decode(generated_mel_spectrogram)\n",
    "    # print(\"taken : \", time.time() - start, \" voice duration : \", duration, generated_wave_form.shape[-1]/24000)\n",
    "    total += time.time() - start\n",
    "print('10번 반복 ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaab5bd-0374-4ed0-9288-62bdc52f942d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181136c-16a2-476c-ab6d-bdeeb110b066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328961ba-5432-4981-90dc-a488ab554d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d8b8b-6e18-4eb2-b9a6-782d20278553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b946f3-6a33-46a9-a86a-6514e67d00b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d38f2-201f-4720-a2be-2051a06b5c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9831e91-69f9-4b05-99c5-f759d51a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "AudioSignal(audio, sample_rate=24000).widget()\n",
    "AudioSignal(generated_wave_form[0].cpu().numpy(), sample_rate=24000).widget()\n",
    "AudioSignal(generated_wave_form[1].cpu().numpy(), sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ceec54-6e80-438a-9953-82f43fae6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute t values using numpy to avoid heavy torch import\n",
    "sway_sampling_coef = -1\n",
    "t = np.linspace(0, 1, 32)\n",
    "t_transformed = t + sway_sampling_coef * (np.cos(np.pi / 2 * t) - 1 + t)\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(t_transformed)\n",
    "plt.xlabel(\"Step Index\")\n",
    "plt.ylabel(\"Transformed t Value\")\n",
    "plt.title(\"Sway Sampling Transformed t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812d149-f907-41d5-9530-e31cab95cefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
