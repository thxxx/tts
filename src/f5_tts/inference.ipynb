{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512afbd4-9333-499d-8609-3dff0d678cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tomli\n",
    "from cached_path import cached_path\n",
    "\n",
    "from model import DiT, CFM\n",
    "import torch\n",
    "from f5_tts.model.utils import (\n",
    "    get_tokenizer,\n",
    "    convert_char_to_pinyin,\n",
    ")\n",
    "import torchaudio\n",
    "from f5_tts.model.modules import MelSpec\n",
    "\n",
    "from infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e6c0cd-bc17-408d-9893-795b7f5f2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "cfg_strength = 3.0\n",
    "scale_phi = 0.75\n",
    "\n",
    "target_sample_rate = 24000\n",
    "mel_spec_type  = \"vocos\"\n",
    "vocoder_name   = \"vocos\"\n",
    "n_mel_channels = 100\n",
    "win_length     = 1024\n",
    "hop_length     = int(win_length//4)\n",
    "n_fft          = 1024\n",
    "target_rms     = 0.1\n",
    "ode_method     = \"euler\"\n",
    "nfe_step       = 32  # 16, 32\n",
    "speed          = 1.0\n",
    "fix_duration   = None\n",
    "vocab_file     = \"./infer/examples/vocab.txt\"\n",
    "tokenizer      = \"custom\"\n",
    "ode_method     = \"euler\"\n",
    "cross_fade_duration = 0.15\n",
    "sway_sampling_coef = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdeb2c06-878d-4c64-9c14-4af81b12f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocos from local path ./vocoder\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "vocoder     = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=\"./vocoder\")\n",
    "transformer = DiT(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cce4d68-2a55-4cec-bc6c-1a350bf6211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "mel_spec = MelSpec(**mel_spec_kwargs)\n",
    "\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    ").to(device)\n",
    "model.eval()\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04907936-41c4-46dc-897b-77cb078e4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "ckpt_path = '/workspace/f5tts_clone_qwen_filter_7.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "# 키 이름에서 \"emewe_model.\" prefix 제거\n",
    "if \"ema_model_state_dict\" in checkpoint:\n",
    "    new_state_dict = {\n",
    "        k.replace(\"ema_model.\", \"\"): v\n",
    "        for k, v in checkpoint['ema_model_state_dict'].items()\n",
    "        if k.startswith(\"ema_model.\")\n",
    "    }\n",
    "\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1323c60-73b9-4694-a383-5640d46d5b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1242799758911133\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(\n",
    "    orig_freq=44100,\n",
    "    new_freq=24000\n",
    ").to(device)\n",
    "\n",
    "tt = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    reference_audio_path = \"/workspace/tts/src/zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3\"\n",
    "    audio, sr = torchaudio.load(reference_audio_path)\n",
    "    audio = resampler(audio.to(device))\n",
    "    tt += time.time() - st\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd7f495-c49a-4a87-bc8c-a6370885da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reference_audio.ndim)\n",
    "\n",
    "# if reference_audio.ndim == 2:\n",
    "#     reference_audio_mel = model.mel_spec(reference_audio)\n",
    "#     reference_audio_mel = reference_audio_mel.permute(0, 2, 1)\n",
    "#     assert reference_audio_mel.shape[-1] == model.num_channels\n",
    "# reference_audio_mel = reference_audio_mel.to(next(model.parameters()).dtype)\n",
    "# print(reference_audio_mel.shape)\n",
    "\n",
    "# ref_text_len = len(reference_script.encode(\"utf-8\"))\n",
    "# gen_text_len = len(\"I love her not bad.\".encode(\"utf-8\"))\n",
    "# duration = reference_audio_len + int(reference_audio_len / ref_text_len * gen_text_len)\n",
    "# print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b403a1b-cb57-4b1b-a418-c0644cadceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.798 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  40,    0, 1082,  441,  827, 1147,  369,  441, 1082,    0,   40,    0,\n",
       "          973,   62, 1149,    0, 1236,  827, 1147,    0,  507,  765,    0,  704,\n",
       "         1236,    0,  250,  940,  325,   62,  704,   14]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from f5_tts.model.utils import (\n",
    "    list_str_to_idx,\n",
    "    list_str_to_tensor,\n",
    ")\n",
    "\n",
    "text = [\"I thought I saw you in my dream.\"]\n",
    "final_text_list = convert_char_to_pinyin(text)\n",
    "\n",
    "list_str_to_idx(text, model.vocab_char_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91aff2ac-0535-4f78-b44b-28a70ff801cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74676])\n",
      "reference_audio_len :  3.1115 s\n",
      "1 tensor([292], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "maximum(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# final_text_list = convert_char_to_pinyin(text)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     generated, sec = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference_audio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg_strength\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43msway_sampling_coef\u001b[49m\u001b[43m=\u001b[49m\u001b[43msway_sampling_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mduplicate_test\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_inter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_ref_audio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(time.time() - st)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts/src/f5_tts/model/cfm.py:114\u001b[39m, in \u001b[36mCFM.sample\u001b[39m\u001b[34m(self, reference_audio, text, duration, steps, cfg_strength, sway_sampling_coef, seed, max_duration, vocoder, no_ref_audio, duplicate_test, t_inter, edit_mask, batch_size)\u001b[39m\n\u001b[32m    112\u001b[39m text = list_str_to_idx(text, \u001b[38;5;28mself\u001b[39m.vocab_char_map).to(device)[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m# 이게 생성할 script의 indexes\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text), cond_length)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m condition_length = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# condition의 최소 길이.\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# duration\u001b[39;00m\n\u001b[32m    117\u001b[39m cond_mask = lens_to_mask(condition_length)\n",
      "\u001b[31mTypeError\u001b[39m: maximum(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "model.eval()\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(\n",
    "    orig_freq=44100,\n",
    "    new_freq=24000\n",
    ").to(device)\n",
    "reference_audio_path = \"/workspace/tts/src/zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3\"\n",
    "reference_script = \"I hate you so much.\"\n",
    "audio, sr = torchaudio.load(reference_audio_path)\n",
    "reference_audio = resampler(audio.to(device))[0].unsqueeze(dim=0)\n",
    "print(reference_audio.shape)\n",
    "\n",
    "reference_audio_len = reference_audio.shape[-1]/24000\n",
    "print(\"reference_audio_len : \", reference_audio_len, \"s\")\n",
    "\n",
    "script = \"I thought I saw you in my dream.\"\n",
    "target_second = 3.0\n",
    "target_duration = int(target_second * 100)\n",
    "# final_text_list = convert_char_to_pinyin(text)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated, sec = model.sample(\n",
    "        reference_audio=reference_audio,\n",
    "        text=script,\n",
    "        duration=target_duration,\n",
    "        steps=16,\n",
    "        cfg_strength=0.0,\n",
    "        sway_sampling_coef=sway_sampling_coef,\n",
    "        \n",
    "        duplicate_test=False,\n",
    "        t_inter=None,\n",
    "        no_ref_audio=False,\n",
    "        batch_size=1\n",
    "    )\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e3618-3d9e-46a7-a35b-a98412b611b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "generated = generated.to(torch.float32)\n",
    "print(generated.shape)\n",
    "generated_mel_spectrogram = generated.permute(0, 2, 1)\n",
    "generated_wave_form = vocoder.decode(generated_mel_spectrogram)\n",
    "print(generated_wave_form.shape)\n",
    "\n",
    "AudioSignal(generated_wave_form.cpu().numpy()[0], sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191a3d7-8b27-451d-93a6-7f69455e514f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ead2a0-29f5-44bc-812d-505711d52af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파라미터 수\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 학습 가능한(=requires_grad=True) 파라미터 수\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2375f2e-668a-4baa-9611-812d709b0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "ref_audio = \"/workspace/tts/valid_data/monster-saying-i-love-you-sound-effect-234404303_nw_prev.mp3\"\n",
    "ref_text = \"I love you.\"\n",
    "\n",
    "audio, sr = librosa.load(ref_audio, sr=24000, mono=True)\n",
    "audio = torch.tensor(audio).unsqueeze(dim=0)\n",
    "print(audio.shape) # 2.18초 = 약 202 frames\n",
    "\n",
    "text = [ref_text+\"I thought I saw you in my dream. model will be optimized by back propagation with tons of data.\", ref_text+\"Hey how are you?\"]\n",
    "duration = 920\n",
    "final_text_list = convert_char_to_pinyin(text)\n",
    "\n",
    "total = 0\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    generated, sec = model.sample(\n",
    "        cond           = audio.to(device),\n",
    "        text           = final_text_list,\n",
    "        duration       = duration,\n",
    "        steps          = 32,\n",
    "        cfg_strength   = cfg_strength,\n",
    "        sway_sampling_coef=-1,\n",
    "        \n",
    "        duplicate_test = False,\n",
    "        t_inter        = None,\n",
    "        no_ref_audio   = False,\n",
    "        batch_size     = 1\n",
    "    )\n",
    "    \n",
    "    generated = generated.to(torch.float32)\n",
    "    generated_mel_spectrogram = generated.permute(0, 2, 1)\n",
    "    generated_wave_form = vocoder.decode(generated_mel_spectrogram)\n",
    "    # print(\"taken : \", time.time() - start, \" voice duration : \", duration, generated_wave_form.shape[-1]/24000)\n",
    "    total += time.time() - start\n",
    "print('10번 반복 ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaab5bd-0374-4ed0-9288-62bdc52f942d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181136c-16a2-476c-ab6d-bdeeb110b066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328961ba-5432-4981-90dc-a488ab554d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d8b8b-6e18-4eb2-b9a6-782d20278553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b946f3-6a33-46a9-a86a-6514e67d00b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d38f2-201f-4720-a2be-2051a06b5c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9831e91-69f9-4b05-99c5-f759d51a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "AudioSignal(audio, sample_rate=24000).widget()\n",
    "AudioSignal(generated_wave_form[0].cpu().numpy(), sample_rate=24000).widget()\n",
    "AudioSignal(generated_wave_form[1].cpu().numpy(), sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ceec54-6e80-438a-9953-82f43fae6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute t values using numpy to avoid heavy torch import\n",
    "sway_sampling_coef = -1\n",
    "t = np.linspace(0, 1, 32)\n",
    "t_transformed = t + sway_sampling_coef * (np.cos(np.pi / 2 * t) - 1 + t)\n",
    "\n",
    "# Plot the result\n",
    "plt.plot(t_transformed)\n",
    "plt.xlabel(\"Step Index\")\n",
    "plt.ylabel(\"Transformed t Value\")\n",
    "plt.title(\"Sway Sampling Transformed t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812d149-f907-41d5-9530-e31cab95cefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
