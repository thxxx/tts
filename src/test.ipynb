{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fad68eed-3453-42b1-b90d-30485d2e5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from x_transformers.x_transformers import RotaryEmbedding\n",
    "\n",
    "from f5_tts.model.modules import (\n",
    "    TimestepEmbedding,\n",
    "    ConvNeXtV2Block,\n",
    "    ConvPositionEmbedding,\n",
    "    DiTBlock,\n",
    "    AdaLayerNormZero_Final,\n",
    "    precompute_freqs_cis,\n",
    "    get_pos_embed_indices,\n",
    ")\n",
    "from torch import nn\n",
    "import torch\n",
    "import logging, warnings\n",
    "import string\n",
    "import typing as tp\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Text embedding\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, text_dim, conv_layers=0, conv_mult=2):\n",
    "        super().__init__()\n",
    "        self.text_embed = nn.Embedding(vocab_size+1, text_dim)  # use 0 as filler token\n",
    "\n",
    "        if conv_layers > 0:\n",
    "            self.extra_modeling = True\n",
    "            self.precompute_max_pos = 4096  # ~44s of 24khz audio\n",
    "            # self.register_buffer(\"freqs_cis\", precompute_freqs_cis(text_dim, self.precompute_max_pos), persistent=False)\n",
    "            self.freqs_cis = precompute_freqs_cis(text_dim, self.precompute_max_pos) # (precompute_max_pos, text_dim)\n",
    "            self.text_blocks = nn.Sequential(\n",
    "                *[ConvNeXtV2Block(text_dim, text_dim * conv_mult) for _ in range(conv_layers)]\n",
    "            )\n",
    "        else:\n",
    "            self.extra_modeling = False\n",
    "\n",
    "    def forward(self, text, seq_len, drop_text=False):  # noqa: F722\n",
    "        # text is tokenized by custom vocab.\n",
    "        text = text + 1  # use 0 as filler token. preprocess of batch pad -1, see list_str_to_idx()\n",
    "        text = text[:, :seq_len]  # curtail if character tokens are more than the mel spec tokens\n",
    "        batch, text_len = text.shape[0], text.shape[1]\n",
    "        text = F.pad(text, (0, seq_len - text_len), value=0)\n",
    "        print(\"padded text \", text.shape)\n",
    "\n",
    "        if drop_text and random.random()<drop_text:  # cfg for text\n",
    "            text = torch.zeros_like(text)\n",
    "\n",
    "        text = self.text_embed(text)  # b n -> b n d\n",
    "\n",
    "        # possible extra modeling\n",
    "        if self.extra_modeling:\n",
    "            # sinus pos emb\n",
    "            batch_start = torch.zeros((batch,), dtype=torch.long)\n",
    "            pos_idx = get_pos_embed_indices(batch_start, seq_len, max_pos=self.precompute_max_pos)\n",
    "            text_pos_embed = self.freqs_cis[pos_idx]\n",
    "            text = text + text_pos_embed\n",
    "\n",
    "            # convnextv2 blocks\n",
    "            text = self.text_blocks(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fce67ed-d5ff-4814-a4c7-905ac5ce7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tts.model.utils import get_tokenizer, list_str_to_idx\n",
    "\n",
    "text_num_embeds=2546\n",
    "text_dim=512\n",
    "conv_layers=4\n",
    "\n",
    "vocab_file = \"./f5_tts/infer/examples/vocab.txt\"\n",
    "tokenizer = \"custom\"\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer) # len(vocab_char_map) = 2545 = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "debee798-07e5-4458-9133-e8fed5487b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "text = [\"hello\", \"it's raining\"]\n",
    "text = list_str_to_idx(text, vocab_char_map)\n",
    "print(text.shape)\n",
    "\n",
    "text_module = TextEmbedding(vocab_size=vocab_size, text_dim=text_dim, conv_layers=conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07612964-5ce7-4ea7-88ad-21c208835951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded text  torch.Size([2, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed = text_module(text, seq_len=1000, drop_text=0.0) # seq_len if for setting max_len\n",
    "\n",
    "text_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bcbcac8-51cb-4dec-8f46-27bc20cf37b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6813,  0.6497,  0.2855,  ..., -1.3252, -0.0537,  0.0632],\n",
       "        [-0.3184, -0.1016,  0.0722,  ..., -1.2688, -0.0505,  0.0849],\n",
       "        [-0.7829, -1.0163, -0.7877,  ..., -1.2776, -0.0253,  0.1682],\n",
       "        [-0.2993, -1.3094, -1.6249,  ..., -1.3394,  0.0140,  0.2614],\n",
       "        [ 0.6841, -0.7223, -1.7785,  ..., -1.4046,  0.0351,  0.2931]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed[0][20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed1e4406-36ef-4ba9-b2af-82629e6d2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6797,  0.6491,  0.2860,  ..., -1.3269, -0.0527,  0.0625],\n",
       "        [-0.3184, -0.1016,  0.0722,  ..., -1.2688, -0.0505,  0.0849],\n",
       "        [-0.7829, -1.0163, -0.7878,  ..., -1.2776, -0.0253,  0.1683],\n",
       "        [-0.2993, -1.3094, -1.6249,  ..., -1.3394,  0.0140,  0.2614],\n",
       "        [ 0.6841, -0.7223, -1.7785,  ..., -1.4046,  0.0351,  0.2931]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed[1][20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17990ce7-c540-451b-8f77-21d0fc045807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f23306-d7db-4965-8098-a51d4ff91441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text :  tensor([[ 442,  326,  616,  ...,    0,    0,    0],\n",
      "        [ 508, 1083,    8,  ...,    0,    0,    0]], device='cuda:0')\n",
      "text :  torch.Size([2, 1000, 512])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 1000\n",
    "\n",
    "# text is tokenized by custom vocab.\n",
    "text = text + 1  # use 0 as filler token. preprocess of batch pad -1, see list_str_to_idx()\n",
    "text = text[:, :seq_len]  # curtail if character tokens are more than the mel spec tokens\n",
    "batch, text_len = text.shape[0], text.shape[1]\n",
    "text = F.pad(text, (0, seq_len - text_len), value=0)\n",
    "print(\"text : \", text)\n",
    "\n",
    "text = text_embed.text_embed(text)  # b n -> b n d\n",
    "print(\"text : \", text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f0f962-636f-4464-aa2e-537cddede408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "precompute_max_pos = 4096\n",
    "\n",
    "batch_start = torch.zeros((batch,), dtype=torch.long)\n",
    "print(batch_start)\n",
    "\n",
    "pos_idx = get_pos_embed_indices(batch_start, seq_len, max_pos=precompute_max_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dae693d-bc0e-41df-b8b7-6e5a1085d483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 997, 998, 999],\n",
       "        [  0,   1,   2,  ..., 997, 998, 999]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dd8609-ed8c-45ec-a322-46d5df87399e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[43mtext_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "text = text + text_embed.freqs_cis[pos_idx].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0a840-3462-48ee-ac44-18443f17eaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "770506b2-6752-4c2b-8e54-afd244c27e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75810d64148f4e41bec61713599e4e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12151175365542b2b75dd55bd5d9f208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69ed4e742f743cca93c10a666e6c4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e85164f7b414671a12c7c6d8a4b2099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tomli\n",
    "from cached_path import cached_path\n",
    "\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "from f5_tts.model import DiT, UNetT, CFM\n",
    "import torch\n",
    "from f5_tts.model.utils import (\n",
    "    get_tokenizer,\n",
    "    convert_char_to_pinyin,\n",
    "    list_str_to_idx,\n",
    "    lens_to_mask,\n",
    "    mask_from_frac_lengths\n",
    ")\n",
    "from audiotools import AudioSignal\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from torch.cuda.amp import autocast\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from f5_tts.model.cfm import T5Conditioner\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "mel_spec_type = \"vocos\"\n",
    "target_rms = 0.1\n",
    "cross_fade_duration = 0.15\n",
    "ode_method = \"euler\"\n",
    "nfe_step = 32  # 16, 32\n",
    "cfg_strength = 2.0\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "fix_duration = None\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "t5_model_name = \"t5-base\"\n",
    "text_conditioner = T5Conditioner(t5_model_name=\"t5-base\", max_length=128).to(device)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision = \"fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f90b36-75e4-4c86-b822-f074436d8d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2545\n",
      "Load vocos from local path ../src/f5_tts/vocoder\n",
      "532889700\n"
     ]
    }
   ],
   "source": [
    "model_cls = DiT\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")\n",
    "\n",
    "vocab_file = \"./f5_tts/infer/examples/vocab.txt\"\n",
    "tokenizer = \"custom\"\n",
    "vocoder_name = \"vocos\"\n",
    "ode_method = \"euler\"\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "print(vocab_size)\n",
    "vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=\"../src/f5_tts/vocoder\")\n",
    "\n",
    "transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)\n",
    "\n",
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    "    frac_lengths_mask=(0.7, 1.0),\n",
    "    audio_drop_prob=0.3,\n",
    "    cond_drop_prob=0.2,\n",
    "    caption_drop_prob=0.2\n",
    ").to(device)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d3a1fa2-a957-4b3b-a57a-393be4203073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'time_embed': TimestepEmbedding(\n",
       "    (time_embed): SinusPositionEmbedding()\n",
       "    (time_mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'text_embed': TextEmbedding(\n",
       "    (text_embed): Embedding(2546, 512)\n",
       "    (text_blocks): Sequential(\n",
       "      (0): ConvNeXtV2Block(\n",
       "        (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
       "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (pwconv1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (grn): GRN()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): ConvNeXtV2Block(\n",
       "        (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
       "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (pwconv1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (grn): GRN()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): ConvNeXtV2Block(\n",
       "        (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
       "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (pwconv1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (grn): GRN()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (3): ConvNeXtV2Block(\n",
       "        (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
       "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (pwconv1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (grn): GRN()\n",
       "        (pwconv2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'input_embed': InputEmbedding(\n",
       "    (proj): Linear(in_features=712, out_features=1024, bias=True)\n",
       "    (conv_pos_embed): ConvPositionEmbedding(\n",
       "      (conv1d): Sequential(\n",
       "        (0): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=16)\n",
       "        (1): Mish()\n",
       "        (2): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), padding=(15,), groups=16)\n",
       "        (3): Mish()\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'rotary_embed': RotaryEmbedding(),\n",
       "  'caption_adaLN_mlp': Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "  ),\n",
       "  'caption_mlp': Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "  ),\n",
       "  'transformer_blocks': ModuleList(\n",
       "    (0-21): 22 x DiTBlock(\n",
       "      (attn_norm): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (to_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (cross_attend_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (cattn): Attention(\n",
       "        (to_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_k_c): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_v_c): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_q_c): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (to_out_c): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (ff_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (ff): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='tanh')\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'norm_out': AdaLayerNormZero_Final(\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
       "  ),\n",
       "  'proj_out': Linear(in_features=1024, out_features=100, bias=True)},\n",
       " 'bottle_dim': 128,\n",
       " 'prepend_length': 32,\n",
       " 'dim': 1024,\n",
       " 'depth': 22,\n",
       " 'long_skip_connection': None}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcb7dfb-229e-485e-869e-c0cfb79dae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"a man saying\"]\n",
    "\n",
    "caption_embed, attention_mask = text_conditioner(text, device=device)\n",
    "\n",
    "caption_embed.shape\n",
    "\n",
    "mel = torch.zeros((1, 100, 100)).to(device)\n",
    "mel_lengths = torch.tensor([100]).to(device)\n",
    "scripts = [\"HHIH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21af0b07-7f90-49c4-9fa2-ea667342b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100]) torch.Size([1, 100, 100]) torch.Size([1, 100, 512])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n",
      "caption_embed :  torch.Size([1024])\n",
      "t :  torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "loss, cond, pred = model(\n",
    "    mel, text=scripts, lens=mel_lengths, noise_scheduler=None, caption_embed=caption_embed, attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f5219e-81eb-442c-abb4-2070abdc74e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17324032\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(768, 1024)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "print(num_trainable_params * 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e482bd75-cbbb-4e08-baee-7ad8a079f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337096804\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3621e-3621-49de-b64a-097a1766ff62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
