{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a1c1dd-b980-4f2d-bd17-347de62bc943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocos from local path ./f5_tts/vocoder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from pathlib import Path\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "from f5_tts.model.cfm import T5Conditioner\n",
    "from f5_tts.model import DiT, CFM\n",
    "from f5_tts.model.utils import get_tokenizer\n",
    "from f5_tts.train.utils import make_html\n",
    "import torchaudio\n",
    "import argparse\n",
    "\n",
    "TRANSFORMERS_NO_TORCHVISION=1\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "cfg_strength = 3.0\n",
    "scale_phi = 0.75\n",
    "ckpt_path = '/workspace/f5tts_clone_qwen_filter_7.pt'\n",
    "\n",
    "mel_spec_type = \"vocos\"\n",
    "vocoder_name = \"vocos\"\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "target_rms = 0.1\n",
    "cross_fade_duration = 0.15\n",
    "ode_method = \"euler\"\n",
    "nfe_step = 32  # 16, 32\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "fix_duration = None\n",
    "vocab_file = '/workspace/tts/ckpts/vocab.txt'\n",
    "tokenizer = \"custom\"\n",
    "ode_method = \"euler\"\n",
    "\n",
    "# load model\n",
    "model_cls = DiT\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=\"./f5_tts/vocoder\")\n",
    "\n",
    "transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)\n",
    "text_conditioner = T5Conditioner(t5_model_name=\"t5-base\", max_length=32).to(device)\n",
    "text_conditioner.eval()\n",
    "\n",
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    ").to(device)\n",
    "\n",
    "dtype = torch.float32\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c3fa18-b02f-4deb-8ac8-27d328eb1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio...\n",
      "Using custom reference text...\n",
      "gen_text 0 Hello everyone\n",
      "Generating audio in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.772 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before start :  torch.Size([1, 69910]) [['I', ' ', 'h', 'a', 't', 'e', ' ', 'y', 'o', 'u', ' ', 's', 'o', ' ', 'm', 'u', 'c', 'h', '.', ' ', ' ', 'H', 'e', 'l', 'l', 'o', ' ', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e']] 455 32 False False 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "script = \"Hello everyone\"\n",
    "prefix_path = \"/workspace/tts/src/zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3\"\n",
    "prefix_script = \"I hate you so much\"\n",
    "\n",
    "if prefix_path is not None and prefix_script is not None:\n",
    "    main_voice = {\"ref_audio\": prefix_path, \"ref_text\": prefix_script}\n",
    "    voices = {\"main\": main_voice}\n",
    "    \n",
    "    for voice in voices:\n",
    "        voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"] = preprocess_ref_audio_text(\n",
    "            voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"]\n",
    "        )\n",
    "    ref_audio = voices[voice][\"ref_audio\"]\n",
    "    ref_text = voices[voice][\"ref_text\"]\n",
    "    no_ref_audio = False\n",
    "else:\n",
    "    ref_audio = None\n",
    "    ref_text = \" \"\n",
    "    no_ref_audio = True\n",
    "\n",
    "audio, final_sample_rate, spectragram = infer_process(\n",
    "    ref_audio,\n",
    "    ref_text,\n",
    "    script, \n",
    "    model, \n",
    "    vocoder, \n",
    "    mel_spec_type=mel_spec_type, \n",
    "    speed=speed,\n",
    "    cfg_strength=cfg_strength,\n",
    "    no_ref_audio=no_ref_audio,\n",
    ")\n",
    "if vocoder_name == \"bigvgan\":\n",
    "    array = torch.stack((torch.tensor(audio), torch.tensor(audio)), dim=0).squeeze()\n",
    "else:\n",
    "    array = torch.stack((torch.tensor(audio).unsqueeze(dim=0), torch.tensor(audio).unsqueeze(dim=0)), dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb19cb6-190c-4344-9bcc-263334c69c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "caption = \"\"\n",
    "script = \"You are not allowed to come here! \"\n",
    "\n",
    "# generate \n",
    "prefix_path = \"/workspace/tts_sfx/src/f5_tts/valid_data/zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3\"\n",
    "prefix_script = \"I hate you so much\"\n",
    "\n",
    "# AudioSignal(prefix_path, sample_rate=24000).to_mono().widget()\n",
    "\n",
    "for i in range(3):\n",
    "    audio = main(script=script)\n",
    "    AudioSignal(audio, sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b28bf-f6c1-4863-886c-51210e9b8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"a monster saying\"\n",
    "script = \"I hate you so much\"\n",
    "prefix_path = \"/workspace/tts_sfx/src/f5_tts/valid_data/zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3\"\n",
    "prefix_script = \"I hate you so much\"\n",
    "\n",
    "AudioSignal(prefix_path, sample_rate=24000).to_mono().widget()\n",
    "for i in range(10):\n",
    "    audio = main(prefix_path, prefix_script, caption, script)\n",
    "    AudioSignal(audio, sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58400-7d20-41ff-9a04-7dfbb0287165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7bb2d-9f21-415a-99ca-9cb6e8756807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "ap = 'zombie-or-monster-says-i-sound-effect-079567563_nw_prev.mp3'\n",
    "AudioSignal(ap).widget()\n",
    "\n",
    "audio, sr = librosa.load(ap, sr=24000)\n",
    "\n",
    "audio = torch.tensor(audio)[24000:48000]\n",
    "print(audio.shape)\n",
    "\n",
    "ms = model.mel_spec(audio.unsqueeze(dim=0))\n",
    "print(ms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c2190-3fd5-42f4-aa36-a66d7a7d2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "mss = ms.permute(0, 2, 1)\n",
    "\n",
    "generated_wave = vocoder.decode(ms.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33812fa7-c61a-4b58-a149-603abfe43d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSignal(generated_wave.cpu().numpy(), sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad5d62-5a48-4d3a-9689-77e45819378e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f013eb43-b461-4df5-8e93-9e82da996142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from pathlib import Path\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "from f5_tts.model.cfm import T5Conditioner\n",
    "from f5_tts.model import DiTPrepend, CFM\n",
    "from f5_tts.model.utils import get_tokenizer\n",
    "from f5_tts.train.utils import make_html\n",
    "import torchaudio\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "cfg_strength = 2.0\n",
    "scale_phi = 0.75\n",
    "\n",
    "mel_spec_type = \"vocos\"\n",
    "vocoder_name = mel_spec_type\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "target_rms = 0.1\n",
    "cross_fade_duration = 0.15\n",
    "ode_method = \"euler\"\n",
    "nfe_step = 32  # 16, 32\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "fix_duration = None\n",
    "vocab_file = \"./f5_tts/infer/examples/vocab.txt\"\n",
    "tokenizer = \"custom\"\n",
    "ode_method = \"euler\"\n",
    "\n",
    "# load model\n",
    "model_cls = DiTPrepend\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d84a534-3665-4381-8070-d073c4a48b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocos from local path ./f5_tts/vocoder\n",
      "Load 2\n",
      "Load 3\n"
     ]
    }
   ],
   "source": [
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "if vocoder_name == \"bigvgan\":\n",
    "    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=False)\n",
    "else:\n",
    "    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=f\"./f5_tts/vocoder\")\n",
    "\n",
    "transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)\n",
    "text_conditioner = T5Conditioner(t5_model_name=\"t5-base\", max_length=32).to(device)\n",
    "text_conditioner.eval()\n",
    "\n",
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "print(\"Load 2\")\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    ").to(device)\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "print(\"Load 3\")\n",
    "ckpt_path = \"/workspace/f5tts_clone_qwen_filter_7.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf3b935-7272-432e-aaea-14c09b377149",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_audio_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m t_inter = \u001b[32m0.0\u001b[39m\n\u001b[32m      3\u001b[39m duplicate_test = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minput_audio_url\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_variation:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Use this voice\u001b[39;00m\n\u001b[32m      6\u001b[39m     file_name = input_audio_url.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m      7\u001b[39m     file_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./src/reflist/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'input_audio_url' is not defined"
     ]
    }
   ],
   "source": [
    "# generate #\n",
    "t_inter = 0.0\n",
    "duplicate_test = False\n",
    "if input_audio_url is not None and not is_variation:\n",
    "    # Use this voice\n",
    "    file_name = input_audio_url.split(\"/\")[-1]\n",
    "    file_path = f'./src/reflist/{file_name}'\n",
    "    download_file(input_audio_url, file_path)\n",
    "    prefix_script = original_script\n",
    "\n",
    "main_voice = {\"ref_audio\": file_path, \"ref_text\": prefix_script}\n",
    "voices = {\"main\": main_voice}\n",
    "for voice in voices:\n",
    "    voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"] = preprocess_ref_audio_text(\n",
    "        voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"]\n",
    "    )\n",
    "\n",
    "audio, final_sample_rate, spectragram = infer_process(\n",
    "    voices[voice][\"ref_audio\"],\n",
    "    voices[voice][\"ref_text\"],\n",
    "    script, \n",
    "    model, \n",
    "    vocoder, \n",
    "    mel_spec_type=mel_spec_type, \n",
    "    speed=speed,\n",
    "    cfg_strength=cfg_strength,\n",
    "    no_ref_audio=False,\n",
    "    scale_phi=scale_phi,\n",
    "    t_inter=t_inter,\n",
    "    duplicate_test=duplicate_test,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "print(\"Adua \", audio.shape)\n",
    "if vocoder_name == \"bigvgan\":\n",
    "    array = torch.stack((torch.tensor(audio), torch.tensor(audio)), dim=0).squeeze()\n",
    "else:\n",
    "    array = torch.stack((torch.tensor(audio).unsqueeze(dim=1), torch.tensor(audio).unsqueeze(dim=1)), dim=1).squeeze()\n",
    "print(array.shape)\n",
    "\n",
    "for idx, aud in enumerate(array.to(dtype).cpu().detach()):\n",
    "    print(aud.shape)\n",
    "    torchaudio.save(\n",
    "        f\"sample_{idx}.wav\", aud, sample_rate=target_sample_rate, channels_first=True\n",
    "    )\n",
    "delete_file(file_path)\n",
    "\n",
    "array = array.to(dtype).cpu().detach().numpy()\n",
    "return array.reshape(-1) # [batch_size, 2, audio_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2a9bf-7f71-44ce-ab6f-8a5fcc57a9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
