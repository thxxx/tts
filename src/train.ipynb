{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d033dc8-4467-4c70-a9e2-25ee5f44fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tomli\n",
    "from cached_path import cached_path\n",
    "\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "from f5_tts.model import DiT, UNetT, CFM\n",
    "from f5_tts.train.utils import draw_plot\n",
    "from f5_tts.train.validation import validate\n",
    "import torch\n",
    "from f5_tts.model.utils import (\n",
    "    get_tokenizer,\n",
    "    convert_char_to_pinyin,\n",
    "    list_str_to_idx,\n",
    "    lens_to_mask,\n",
    "    mask_from_frac_lengths\n",
    ")\n",
    "from audiotools import AudioSignal\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from torch.cuda.amp import autocast\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from f5_tts.model.cfm import T5Conditioner\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "mel_spec_type = \"vocos\"\n",
    "target_rms = 0.1\n",
    "cross_fade_duration = 0.15\n",
    "ode_method = \"euler\"\n",
    "nfe_step = 32  # 16, 32\n",
    "cfg_strength = 2.0\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "fix_duration = None\n",
    "preset = \"/workspace/tts_sfx\"\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "t5_model_name = \"t5-base\"\n",
    "text_conditioner = T5Conditioner(t5_model_name=\"t5-base\", max_length=128).to(device)\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d39e8f-670a-482a-b80a-67eb037f68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = DiT\n",
    "model_cfg = dict(\n",
    "    dim=1024, \n",
    "    depth=22, \n",
    "    heads=16, \n",
    "    ff_mult=2, \n",
    "    text_dim=512, \n",
    "    conv_layers=4\n",
    ")\n",
    "\n",
    "vocab_file = \"./f5_tts/infer/examples/vocab.txt\"\n",
    "tokenizer = \"custom\"\n",
    "vocoder_name = \"vocos\"\n",
    "ode_method = \"euler\"\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n",
    "print(vocab_size)\n",
    "vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=True, local_path=f\"{preset}/src/f5_tts/vocoder\")\n",
    "\n",
    "transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels)\n",
    "\n",
    "mel_spec_kwargs=dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "odeint_kwargs=dict(\n",
    "    method=ode_method,\n",
    ")\n",
    "\n",
    "model = CFM(\n",
    "    transformer=transformer,\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    odeint_kwargs=odeint_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    "    frac_lengths_mask=(0.7, 1.0),\n",
    "    audio_drop_prob=0.3,\n",
    "    cond_drop_prob=0.2,\n",
    "    caption_drop_prob=0.2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91840a11-9afd-4c1e-90b3-bcb3aa8cddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained weights\n",
    "dtype = (\n",
    "    torch.float16 if \"cuda\" in device and torch.cuda.get_device_properties(device).major >= 6 else torch.float32\n",
    ")\n",
    "ckpt_path = f\"{preset}/ckpts/model_1200000.pt\"\n",
    "print(dtype)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "\n",
    "checkpoint[\"model_state_dict\"] = {\n",
    "    k.replace(\"ema_model.\", \"\"): v\n",
    "    for k, v in checkpoint[\"ema_model_state_dict\"].items()\n",
    "    if k not in [\"initted\", \"step\"]\n",
    "}\n",
    "\n",
    "# patch for backward compatibility, 305e3ea\n",
    "for key in [\"mel_spec.mel_stft.mel_scale.fb\", \"mel_spec.mel_stft.spectrogram.window\"]:\n",
    "    if key in checkpoint[\"model_state_dict\"]:\n",
    "        del checkpoint[\"model_state_dict\"][key]\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e538f8-b106-4138-849b-708345555595",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size   = 8\n",
    "lr           = 0.00001\n",
    "weight_decay = 0.001\n",
    "betas        = (0.9, 0.999)\n",
    "sample_rate  = 24000\n",
    "train_duration  = 30.0\n",
    "\n",
    "num_workers = 8\n",
    "num_epochs = 100\n",
    "\n",
    "output_dir   = 'weights_1125'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ab16a-dfad-420f-a1a6-de2c5e02a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tts.custom.dataset import CustomDataset, collate_fn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    f\"{preset}/datas/train_expresso.csv\",\n",
    "    target_sample_rate = target_sample_rate,\n",
    "    mode           = \"train\",\n",
    "    hop_length     = hop_length,\n",
    "    n_mel_channels = n_mel_channels,\n",
    "    win_length     = win_length,\n",
    "    n_fft          = n_fft,\n",
    "    mel_spec_type  = \"vocos\",\n",
    "    preprocessed_mel = False,\n",
    "    mel_spec_module = None\n",
    ")\n",
    "valid_dataset = CustomDataset(\n",
    "    f\"{preset}/datas/valid_expresso.csv\",\n",
    "    target_sample_rate = target_sample_rate,\n",
    "    mode           = \"validation\",\n",
    "    hop_length     = hop_length,\n",
    "    n_mel_channels = n_mel_channels,\n",
    "    win_length     = win_length,\n",
    "    n_fft          = n_fft,\n",
    "    mel_spec_type  = \"vocos\",\n",
    "    preprocessed_mel = False,\n",
    "    mel_spec_module = None\n",
    ")\n",
    "\n",
    "# Define Train Sampler\n",
    "steps_per_epoch = 3001\n",
    "num_samples_per_epoch = steps_per_epoch * batch_size  # 총 샘플 수 = 스텝 수 * 배치 크기 # train 320,000 samples per epoch\n",
    "train_sampler = RandomSampler(train_dataset, replacement=True, num_samples=num_samples_per_epoch)  # Train Sampler with replacement\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,  # Shuffle validation data without sampler\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "libri_valid_dataset = CustomDataset(\n",
    "    f\"{preset}/datas/libri_test_clean.csv\",\n",
    "    target_sample_rate = target_sample_rate,\n",
    "    mode           = \"validation\",\n",
    "    hop_length     = hop_length,\n",
    "    n_mel_channels = n_mel_channels,\n",
    "    win_length     = win_length,\n",
    "    n_fft          = n_fft,\n",
    "    mel_spec_type  = \"vocos\",\n",
    "    preprocessed_mel = False,\n",
    "    mel_spec_module  = None\n",
    ")\n",
    "libri_valid_loader = DataLoader(libri_valid_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92c0c5-1fc4-4fbe-a9e1-c551a325172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "\n",
    "trainer = {\n",
    "    'train_losses': [],\n",
    "    'valid_losses': [],\n",
    "    'libri_valid_losses': [],\n",
    "    'wer': [],\n",
    "    'cer': [],\n",
    "    'lrs': [],\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas)\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=21, # 20 epoch마다 1/10이 된다.\n",
    "    cycle_mult=1.0,\n",
    "    max_lr=lr,\n",
    "    min_lr=lr/10,\n",
    "    warmup_steps=20,\n",
    "    gamma=0.8 # 한 사이클 돌 때마다 max lr이 80%가 된다.\n",
    ")\n",
    "\n",
    "noise_scheduler = None # 지금 세팅 안되어있음\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# model, train_loader, text_conditioner, valid_loader, libri_valid_loader = accelerator.prepare(model, train_loader, text_conditioner, valid_loader, libri_valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760be63-2448-40d0-928e-f78730be0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "print(\"start training\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    text_conditioner.eval()\n",
    "    epoch_loss = 0\n",
    "    tqdm_bar = tqdm(total=len(train_loader), desc=\"F5-TTS Training\")\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        mel = batch[\"mel\"].to(device)\n",
    "        mel_lengths = batch[\"mel_lengths\"].to(device)\n",
    "        scripts = batch[\"script\"]\n",
    "        caption = batch[\"caption\"]\n",
    "        mel_spec = mel.permute(0, 2, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # if random.random()<0.2:\n",
    "            #     caption_embed, attention_mask = None, None\n",
    "            # else:\n",
    "            caption_embed, attention_mask = text_conditioner(caption, device=device)\n",
    "\n",
    "        with autocast():\n",
    "            loss = model(\n",
    "                mel_spec, text=scripts, lens=mel_lengths, noise_scheduler=noise_scheduler, caption_embed=caption_embed, attention_mask=attention_mask\n",
    "            )\n",
    "        loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "        \n",
    "        # if max_grad_norm > 0 and accelerator.sync_gradients:\n",
    "        #     accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        tqdm_bar.update()\n",
    "        # epoch_loss += loss\n",
    "        epoch_loss += loss.cpu().detach().item()\n",
    "        del loss\n",
    "        del caption_embed\n",
    "        del attention_mask\n",
    "        \n",
    "        if idx%1000 == 999:\n",
    "            scheduler.step()\n",
    "            trainer['train_losses'].append(epoch_loss/idx)\n",
    "            trainer['lrs'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            draw_plot('train_losses', trainer, output_dir=output_dir)\n",
    "            draw_plot('lrs', trainer, output_dir=output_dir)\n",
    "            # 텍스트 파일에 쓰기\n",
    "            with open(f'./{output_dir}/middle_logs.txt', 'a') as file:\n",
    "                file.write(f\"\\nEpoch - {epoch} : {epoch_loss/idx}\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(epoch_loss/idx)\n",
    "    with open(f'./{output_dir}/middle_logs.txt', 'a') as file:\n",
    "        file.write(f\"\\nEpoch - {epoch} : {epoch_loss/idx}\\n\")\n",
    "    scheduler.step()\n",
    "    trainer['train_losses'].append(epoch_loss/idx)\n",
    "    trainer['lrs'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    draw_plot('train_losses', trainer, output_dir=output_dir)\n",
    "    draw_plot('lrs', trainer, output_dir=output_dir)\n",
    "\n",
    "    model.eval()\n",
    "    unwrapped_model = model\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    torch.save(unwrapped_model.state_dict(), f'./{output_dir}/model_{epoch}.pt')\n",
    "\n",
    "    valid_expresso_loss, wer, cer = validate(\n",
    "        unwrapped_model,\n",
    "        valid_loader,\n",
    "        vocoder,\n",
    "        text_conditioner,\n",
    "        output_dir=output_dir,\n",
    "        epoch=epoch,\n",
    "        noise_scheduler=None,\n",
    "        is_caption=True,\n",
    "        is_make_samples=True,\n",
    "        is_whisper=True,\n",
    "        device='cuda'\n",
    "    )\n",
    "    valid_libri_loss, _, _ = validate(\n",
    "        unwrapped_model,\n",
    "        libri_valid_loader,\n",
    "        vocoder,\n",
    "        text_conditioner,\n",
    "        output_dir=output_dir,\n",
    "        epoch=epoch,\n",
    "        noise_scheduler=None,\n",
    "        is_caption=False,\n",
    "        is_make_samples=False,\n",
    "        is_whisper=False,\n",
    "        device='cuda'\n",
    "    )\n",
    "    \n",
    "    trainer['valid_losses'].append(valid_expresso_loss)\n",
    "    trainer['libri_valid_losses'].append(valid_libri_loss)\n",
    "    \n",
    "    draw_plot('valid_losses', trainer, output_dir=output_dir)\n",
    "    draw_plot('libri_valid_losses', trainer, output_dir=output_dir)\n",
    "    \n",
    "    if is_make_samples:\n",
    "        trainer['clap_score'].append(clap_score)\n",
    "        trainer['wer'].append(wer)\n",
    "        trainer['cer'].append(cer)\n",
    "        \n",
    "        draw_plot('clap_score', trainer, output_dir=output_dir)\n",
    "        draw_plot('wer', trainer, output_dir=output_dir)\n",
    "        draw_plot('cer', trainer, output_dir=output_dir)\n",
    "    \n",
    "    # 텍스트 파일에 쓰기\n",
    "    with open(f'./{output_dir}/logs.txt', 'a') as file:\n",
    "        file.write(f\"\\nEpoch - {epoch}\\n\")\n",
    "        file.write(f\"Train loss : {epoch_loss/len(train_loader)}\\n\")\n",
    "        file.write(f\"valid_expresso_loss : {valid_expresso_loss}\\n\")\n",
    "        file.write(f\"valid_libri_loss : {valid_libri_loss}\\n\")\n",
    "        \n",
    "        if is_make_samples:\n",
    "            file.write(f\"Clap score : {clap_score}\\n\")\n",
    "            file.write(f\"Wer : {wer}\\n\")\n",
    "            file.write(f\"Cer : {cer}\\n\")\n",
    "\n",
    "    del unwrapped_model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae7b8c-e801-4365-a0d0-3f760390a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478a6e4-7b7d-43d8-91b2-891e8c988b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbec372-9b81-47d8-b4ea-40f5324171f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    ")\n",
    "from f5_tts.train.custom_prompts import custom_prompts, creature_prompts\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from f5_tts.train.utils import make_html\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from jiwer import wer\n",
    "from difflib import SequenceMatcher\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    # Word Error Rate 계산\n",
    "    return wer(reference, hypothesis)\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    # Character Error Rate 계산\n",
    "    # 레벤슈타인 거리 기반으로 일치 비율 계산\n",
    "    matcher = SequenceMatcher(None, reference, hypothesis)\n",
    "    cer = 1 - matcher.ratio()\n",
    "    return cer\n",
    "\n",
    "def validate(model, valid_loader, vocoder, text_conditioner, output_dir, epoch, noise_scheduler, is_caption=False, is_make_samples=False, is_whisper=False, device='cuda'):\n",
    "    model.eval()\n",
    "    valid_loss=0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(valid_loader):\n",
    "            mel = batch[\"mel\"].to(device)\n",
    "            mel_lengths = batch[\"mel_lengths\"].to(device)\n",
    "            scripts = batch[\"script\"]\n",
    "            caption = batch[\"caption\"]\n",
    "            mel_spec = mel.permute(0, 2, 1).to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                if is_caption:\n",
    "                    caption_embed, attention_mask = text_conditioner(caption, device=device)\n",
    "                else:\n",
    "                    caption_embed, attention_mask = None, None\n",
    "                loss, cond, pred = model(\n",
    "                    mel_spec, text=scripts, lens=mel_lengths, noise_scheduler=noise_scheduler, caption_embed=caption_embed, attention_mask=attention_mask\n",
    "                )\n",
    "            valid_loss += loss\n",
    "        print(valid_loss)\n",
    "\n",
    "    wer = 0\n",
    "    cer = 0\n",
    "    if is_make_samples:\n",
    "        speed = 1.0\n",
    "        mel_spec_type = \"vocos\"\n",
    "\n",
    "        output_list = []\n",
    "        for data in custom_prompts:\n",
    "            caption = data['text']\n",
    "            script = data['script']\n",
    "            caption_embed, attention_mask = text_conditioner(caption, device=device)\n",
    "            audio, final_sample_rate, spectragram = infer_process(\n",
    "                None,\n",
    "                \"kill all. \", # 1초 짜리 zeros가 들어가니까\n",
    "                script, \n",
    "                model, \n",
    "                vocoder, \n",
    "                mel_spec_type=mel_spec_type, \n",
    "                speed=speed,\n",
    "                no_ref_audio=True,\n",
    "                caption_embed=caption_embed,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            output_list.append({\n",
    "                'array': torch.stack((torch.tensor(audio).unsqueeze(dim=0), torch.tensor(audio).unsqueeze(dim=0)), dim=0).squeeze(),\n",
    "                'caption': caption + \" - \" + script,\n",
    "                'script': script\n",
    "            })\n",
    "        \n",
    "        for data in creature_prompts:\n",
    "            prefix_path = data['prefix_path']\n",
    "            prefix_script = data['prefix_script']\n",
    "            caption = data['caption']\n",
    "            script = data['script']\n",
    "\n",
    "            main_voice = {\n",
    "                \"ref_audio\": prefix_path,\n",
    "                \"ref_text\": prefix_script,\n",
    "            }\n",
    "            \n",
    "            voices = {\n",
    "                \"main\": main_voice\n",
    "            }\n",
    "            for voice in voices:\n",
    "                voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"] = preprocess_ref_audio_text(\n",
    "                    voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"]\n",
    "                )\n",
    "            \n",
    "            caption_embed, attention_mask = text_conditioner(caption, device=device)\n",
    "            \n",
    "            audio, final_sample_rate, spectragram = infer_process(\n",
    "                voices[voice][\"ref_audio\"],\n",
    "                voices[voice][\"ref_text\"],\n",
    "                script, \n",
    "                model, \n",
    "                vocoder, \n",
    "                mel_spec_type=mel_spec_type, \n",
    "                speed=speed,\n",
    "                no_ref_audio=True,\n",
    "                caption_embed=caption_embed,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            print(audio.shape)\n",
    "            output_list.append({\n",
    "                'array': torch.stack((torch.tensor(audio).unsqueeze(dim=0), torch.tensor(audio).unsqueeze(dim=0)), dim=0).squeeze(),\n",
    "                'caption': \"prefix + \" + caption + \" - \" + script,\n",
    "                'script': script\n",
    "            })\n",
    "\n",
    "        make_html(epoch, output_dir, output_list)\n",
    "        \n",
    "        if is_whisper:\n",
    "            # load model and processor\n",
    "            whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "            whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\").to(device)\n",
    "            whisper_model.config.forced_decoder_ids = None\n",
    "        \n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=24000, new_freq=16000)\n",
    "            for i in range(len(output_list)):\n",
    "                audio = output_list[i]['array'].squeeze()[0]\n",
    "                script = output_list[i]['script']\n",
    "                resampled_audio = resampler(audio)\n",
    "                \n",
    "                input_features = whisper_processor(resampled_audio, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "                predicted_ids = whisper_model.generate(input_features)\n",
    "                transcription = whisper_processor.batch_decode(predicted_ids.cpu(), skip_special_tokens=True)[0]\n",
    "                \n",
    "                wer_result = calculate_wer(script, transcription)\n",
    "                cer_result = calculate_cer(script, transcription)\n",
    "                wer += wer_result\n",
    "                cer += cer_result\n",
    "    \n",
    "            del whisper_processor\n",
    "            del whisper_model\n",
    "            \n",
    "\n",
    "    return valid_loss.cpu().detach().item()/len(valid_loader), wer, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad331d-4efa-4e54-bb26-7ba0a235d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = validate(\n",
    "    model,\n",
    "    valid_loader,\n",
    "    vocoder,\n",
    "    text_conditioner,\n",
    "    output_dir=\"weights_1125\",\n",
    "    epoch=0,\n",
    "    noise_scheduler=None,\n",
    "    is_caption=False,\n",
    "    is_make_samples=True,\n",
    "    is_whisper=True,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd5958-8f76-4f43-bf3a-7028acf35dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758bff3-5a4d-4b71-9058-e03dc4a602c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910abd1-517b-4f9b-998a-13355317f171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb64dc-95f4-4705-85ec-6b0ac29fef36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e28c4-a613-46cd-984f-28e6575e5555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
